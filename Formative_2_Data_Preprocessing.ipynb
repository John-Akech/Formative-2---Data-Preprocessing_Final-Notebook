{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPmaYvcTd0KE16HoNdbfHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-Akech/Formative-2---Data-Preprocessing_Final-Notebook/blob/master/Formative_2_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Data Augmentation on CSV Files\n",
        "\n",
        "Objective: Expand an existing dataset using synthetic data, perturbation, and augmentation techniques."
      ],
      "metadata": {
        "id": "HfI7oGr5TfuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning & Handling Missing Values"
      ],
      "metadata": {
        "id": "HetAq58MSD2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import uuid\n",
        "import datetime"
      ],
      "metadata": {
        "id": "dnFv0UAhTeYx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dataset\n",
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('/content/customer_transactions.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'customer_transactions.csv' was not found.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "bdoZcsNQTuEG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information\n",
        "print(\"\\nDataset Overview:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsheHTRUTuCT",
        "outputId": "c8984673-2f11-43c6-d9c4-3df5041d3215"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Overview:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   customer_id_legacy  150 non-null    int64  \n",
            " 1   transaction_id      150 non-null    int64  \n",
            " 2   purchase_amount     150 non-null    int64  \n",
            " 3   purchase_date       150 non-null    object \n",
            " 4   product_category    150 non-null    object \n",
            " 5   customer_rating     140 non-null    float64\n",
            "dtypes: float64(1), int64(3), object(2)\n",
            "memory usage: 7.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows\n",
        "print(\"\\nFirst 5 Rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_-Yzm81TuAJ",
        "outputId": "302822be-a5ef-478a-f919-498a1b98768f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 Rows:\n",
            "   customer_id_legacy  transaction_id  purchase_amount purchase_date  \\\n",
            "0                 151            1001              408    2024-01-01   \n",
            "1                 192            1002              332    2024-01-02   \n",
            "2                 114            1003              442    2024-01-03   \n",
            "3                 171            1004              256    2024-01-04   \n",
            "4                 160            1005               64    2024-01-05   \n",
            "\n",
            "  product_category  customer_rating  \n",
            "0           Sports              2.3  \n",
            "1      Electronics              4.2  \n",
            "2      Electronics              2.1  \n",
            "3         Clothing              2.8  \n",
            "4         Clothing              1.3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure Correct Data Types\n",
        "numerical_cols = ['purchase_amount', 'customer_rating']\n",
        "categorical_cols = ['product_category']"
      ],
      "metadata": {
        "id": "NXhReYM4Tt73"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert purchase_date to datetime\n",
        "df['purchase_date'] = pd.to_datetime(df['purchase_date'], errors='coerce')"
      ],
      "metadata": {
        "id": "aN49c1vxTt5u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract year, month, and day from purchase_date\n",
        "df['year'] = df['purchase_date'].dt.year\n",
        "df['month'] = df['purchase_date'].dt.month\n",
        "df['day'] = df['purchase_date'].dt.day"
      ],
      "metadata": {
        "id": "U8Xp2tw7Tt3L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the original purchase_date column\n",
        "df.drop(columns=['purchase_date'], inplace=True)"
      ],
      "metadata": {
        "id": "PWbNbSDOTt0_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure numerical columns are numeric\n",
        "for col in numerical_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')# Ensure categorical columns are category type\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype('category')"
      ],
      "metadata": {
        "id": "7fQuIqrTTtzb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify updated data types\n",
        "print(\"\\nUpdated Data Types:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I_9sG7hTtyD",
        "outputId": "ad813a89-6c1b-4444-af59-cf83fd2fda81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Data Types:\n",
            "customer_id_legacy       int64\n",
            "transaction_id           int64\n",
            "purchase_amount          int64\n",
            "product_category      category\n",
            "customer_rating        float64\n",
            "year                     int32\n",
            "month                    int32\n",
            "day                      int32\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle Missing Values\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values Summary:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uXeCgKgTtuW",
        "outputId": "d92c479c-2581-41f1-b6c4-777cbc94b2ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Values Summary:\n",
            "customer_id_legacy     0\n",
            "transaction_id         0\n",
            "purchase_amount        0\n",
            "product_category       0\n",
            "customer_rating       10\n",
            "year                   0\n",
            "month                  0\n",
            "day                    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute numerical columns using median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numerical_cols + ['year', 'month', 'day']] = imputer.fit_transform(df[numerical_cols + ['year', 'month', 'day']])"
      ],
      "metadata": {
        "id": "pxctmZAbTtr_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute categorical columns using mode\n",
        "for col in categorical_cols:\n",
        "    mode_value = df[col].mode()[0]\n",
        "    df[col].fillna(mode_value, inplace=True)\n",
        "    print(f\"\\nFilled missing values in {col} with mode: {mode_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGwY_Q4cUJV0",
        "outputId": "a89997bf-79e1-4845-e2e2-0e5ddbc34cd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filled missing values in product_category with mode: Sports\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-b5588db212dd>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(mode_value, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictive modeling for remaining missing values in customer_rating\n",
        "if df['customer_rating'].isnull().any():\n",
        "    X_train = df[df['customer_rating'].notnull()][['purchase_amount']]\n",
        "    y_train = df[df['customer_rating'].notnull()]['customer_rating']\n",
        "    X_missing = df[df['customer_rating'].isnull()][['purchase_amount']]\n",
        "\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(X_train, y_train)\n",
        "    df.loc[df['customer_rating'].isnull(), 'customer_rating'] = regressor.predict(X_missing)"
      ],
      "metadata": {
        "id": "FHppzAEsUJTi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify missing values are handled\n",
        "print(\"\\nMissing Values After Imputation:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZScoW9TUJRq",
        "outputId": "a2dd94bc-c0f9-4a78-ef82-4dcda016a14d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Values After Imputation:\n",
            "customer_id_legacy    0\n",
            "transaction_id        0\n",
            "purchase_amount       0\n",
            "product_category      0\n",
            "customer_rating       0\n",
            "year                  0\n",
            "month                 0\n",
            "day                   0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation Strategies"
      ],
      "metadata": {
        "id": "nZJa7_X8TVZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Random Noise to Numerical Columns\n",
        "# Add random noise to purchase_amount\n",
        "noise_factor = 0.05  # Adjust noise factor for better augmentation\n",
        "original_std = df['purchase_amount'].std()\n",
        "df['purchase_amount'] += np.random.normal(0, noise_factor * original_std, df.shape[0])\n",
        "\n",
        "print(\"\\nRandom Noise Applied to purchase_amount:\")\n",
        "print(df[['purchase_amount']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50msN4u5URYw",
        "outputId": "f030fb19-750c-4b14-f833-885c953cef1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Noise Applied to purchase_amount:\n",
            "   purchase_amount\n",
            "0       396.610631\n",
            "1       331.379468\n",
            "2       425.465727\n",
            "3       257.868081\n",
            "4        55.457181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Skewed Features\n",
        "# Check skewness\n",
        "skewness = df['purchase_amount'].skew()\n",
        "print(f\"\\nSkewness of purchase_amount: {skewness}\")\n",
        "\n",
        "# Apply log transformation if skewed\n",
        "if skewness > 1:\n",
        "    df['purchase_amount'] = np.log1p(df['purchase_amount'])\n",
        "    print(\"\\nLog Transformation Applied to purchase_amount.\")\n",
        "\n",
        "print(\"\\nTransformed purchase_amount:\")\n",
        "print(df[['purchase_amount']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm6r5YvnURVo",
        "outputId": "d6ddc0c5-84d3-4bca-a944-c86d5da4ffdb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skewness of purchase_amount: 0.05547951416809389\n",
            "\n",
            "Transformed purchase_amount:\n",
            "   purchase_amount\n",
            "0       396.610631\n",
            "1       331.379468\n",
            "2       425.465727\n",
            "3       257.868081\n",
            "4        55.457181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Synthetic Data (Choose One Approach)\n",
        "\n",
        "# Approach 1: Discretize Target Variable and Use SMOTE\n",
        "def augment_with_smote(df):\n",
        "    # Encode categorical variables\n",
        "    X = df.drop(columns=['customer_id_legacy', 'transaction_id', 'customer_rating'])\n",
        "    y = df['customer_rating']\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "    X_encoded = encoder.fit_transform(X[categorical_cols])\n",
        "    X_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "    X_final = pd.concat([X.drop(columns=categorical_cols), X_encoded], axis=1)\n",
        "\n",
        "    # Discretize the continuous target into bins\n",
        "    discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "    y_discrete = discretizer.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Apply SMOTE to the discretized target\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_final, y_discrete)\n",
        "\n",
        "    # Decode the discretized target back to the original scale\n",
        "    y_resampled_continuous = discretizer.inverse_transform(y_resampled.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # Combine resampled data into a DataFrame\n",
        "    synthetic_data = pd.DataFrame(X_resampled, columns=X_final.columns)\n",
        "    synthetic_data['customer_rating'] = y_resampled_continuous\n",
        "\n",
        "    # Decode one-hot-encoded features back to original format\n",
        "    def decode_one_hot(encoded_df, original_df):\n",
        "        decoded_df = pd.DataFrame(index=encoded_df.index)\n",
        "        for col in categorical_cols:\n",
        "            one_hot_cols = [c for c in encoded_df.columns if c.startswith(col)]\n",
        "            decoded_df[col] = encoded_df[one_hot_cols].idxmax(axis=1).str.replace(f\"{col}_\", \"\")\n",
        "        return decoded_df\n",
        "\n",
        "    decoded_categoricals = decode_one_hot(synthetic_data, X)\n",
        "    synthetic_data = pd.concat([synthetic_data.drop(columns=[c for c in synthetic_data.columns if any(cat in c for cat in categorical_cols)]), decoded_categoricals], axis=1)\n",
        "\n",
        "    # Generate synthetic IDs\n",
        "    synthetic_data['customer_id_legacy'] = [uuid.uuid4().int % 10**9 for _ in range(synthetic_data.shape[0])]\n",
        "    synthetic_data['transaction_id'] = range(df['transaction_id'].max() + 1, df['transaction_id'].max() + 1 + synthetic_data.shape[0])\n",
        "\n",
        "    return synthetic_data"
      ],
      "metadata": {
        "id": "NEpRnqnOURSw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 2: Interpolation for Continuous Target\n",
        "def augment_with_interpolation(df):\n",
        "    # Function to generate synthetic samples via interpolation\n",
        "    def interpolate_data(X, y, n_samples):\n",
        "        synthetic_X = []\n",
        "        synthetic_y = []\n",
        "\n",
        "        for _ in range(n_samples):\n",
        "            # Randomly select two samples\n",
        "            idx1, idx2 = np.random.choice(len(X), size=2, replace=False)\n",
        "            alpha = np.random.uniform(0, 1)  # Interpolation factor\n",
        "\n",
        "            # Interpolate features and target\n",
        "            synthetic_X.append(alpha * X.iloc[idx1] + (1 - alpha) * X.iloc[idx2])\n",
        "            synthetic_y.append(alpha * y.iloc[idx1] + (1 - alpha) * y.iloc[idx2])\n",
        "\n",
        "        synthetic_X = pd.DataFrame(synthetic_X, columns=X.columns)\n",
        "        synthetic_y = pd.Series(synthetic_y, name=y.name)\n",
        "        return synthetic_X, synthetic_y\n",
        "\n",
        "    # Prepare data for augmentation\n",
        "    X = df.drop(columns=['customer_id_legacy', 'transaction_id', 'customer_rating'])\n",
        "    y = df['customer_rating']\n",
        "\n",
        "    # Generate synthetic data\n",
        "    n_synthetic_samples = len(df)  # Generate as many synthetic samples as the original dataset\n",
        "    synthetic_X, synthetic_y = interpolate_data(X, y, n_synthetic_samples)\n",
        "\n",
        "    # Combine synthetic data into a DataFrame\n",
        "    synthetic_data = pd.concat([synthetic_X, synthetic_y], axis=1)\n",
        "    synthetic_data.columns = X.columns.tolist() + ['customer_rating']\n",
        "\n",
        "    # Generate synthetic IDs\n",
        "    synthetic_data['customer_id_legacy'] = [uuid.uuid4().int % 10**9 for _ in range(synthetic_data.shape[0])]\n",
        "    synthetic_data['transaction_id'] = range(df['transaction_id'].max() + 1, df['transaction_id'].max() + 1 + synthetic_data.shape[0])\n",
        "\n",
        "    return synthetic_data"
      ],
      "metadata": {
        "id": "w8PMu_8tURPr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an augmentation approach\n",
        "augmentation_method = \"smote\"  # Change to \"interpolation\" if needed\n",
        "\n",
        "if augmentation_method == \"smote\":\n",
        "    synthetic_data = augment_with_smote(df)\n",
        "elif augmentation_method == \"interpolation\":\n",
        "    synthetic_data = augment_with_interpolation(df)\n",
        "\n",
        "# Concatenate synthetic data with the original dataset\n",
        "df_augmented = pd.concat([df, synthetic_data], axis=0).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSynthetic Data Generated:\")\n",
        "print(synthetic_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2BmnKCjURMB",
        "outputId": "394f28b1-d2c1-42f6-ab92-d39b0d392d16"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synthetic Data Generated:\n",
            "   purchase_amount    year  month  day  customer_rating product_category  \\\n",
            "0       396.610631  2024.0    1.0  1.0         1.666667           Sports   \n",
            "1       331.379468  2024.0    1.0  2.0         4.333333      Electronics   \n",
            "2       425.465727  2024.0    1.0  3.0         1.666667      Electronics   \n",
            "3       257.868081  2024.0    1.0  4.0         3.000000         Clothing   \n",
            "4        55.457181  2024.0    1.0  5.0         1.666667         Clothing   \n",
            "\n",
            "   customer_id_legacy  transaction_id  \n",
            "0           974918933            1151  \n",
            "1           131039049            1152  \n",
            "2           376203150            1153  \n",
            "3           561642990            1154  \n",
            "4           436704860            1155  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export the Augmented Data"
      ],
      "metadata": {
        "id": "3Xj5BFHFTWMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the Augmented Dataset\n",
        "\n",
        "# Save the final dataset\n",
        "output_file = 'customer_transactions_augmented.csv.'\n",
        "df_augmented.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nFinal Preprocessed Dataset Saved Successfully as '{output_file}'!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Az3RpyVTcsL",
        "outputId": "34039611-be4b-42bf-eab7-cd5a4df575e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmented Dataset Saved Successfully as 'customer_transactions_augmented_20250316_190135.csv'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Part 2: Merging Datasets with Transitive Properties\n",
        "\n",
        " Objective: Merge two different datasets with shared but indirect relationships between entities."
      ],
      "metadata": {
        "id": "lqCIPJbNUiU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "7fX2TJxMUy-c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Datasets\n",
        "try:\n",
        "    # Load the augmented transactions dataset\n",
        "    transactions_df = pd.read_csv('/content/customer_transactions_augmented.csv.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'customer_transactions_augmented.csv' was not found.\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBhQUnQAU18M",
        "outputId": "ae4d24b6-3060-4420-d584-4674cdba02d0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file 'customer_transactions_augmented.csv' was not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the social profiles dataset\n",
        "    social_profiles_df = pd.read_csv('/content/customer_social_profiles.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'customer_social_profiles.csv' was not found.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "O8ae2Ef4U-eV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the ID mapping dataset\n",
        "    id_mapping_df = pd.read_csv('/content/id_mapping.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'id_mapping.csv' was not found.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "UJppn35mVLYA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about all datasets\n",
        "print(\"\\nTransactions Dataset Overview:\")\n",
        "print(transactions_df.info())\n",
        "print(\"\\nSocial Profiles Dataset Overview:\")\n",
        "print(social_profiles_df.info())\n",
        "print(\"\\nID Mapping Dataset Overview:\")\n",
        "print(id_mapping_df.info())"
      ],
      "metadata": {
        "id": "LhR809PrVOgi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map IDs Using id_mapping.csv\n",
        "# Merge transactions_df with id_mapping_df to get customer_id_new\n",
        "transactions_mapped = pd.merge(\n",
        "    transactions_df,\n",
        "    id_mapping_df,\n",
        "    left_on='customer_id_legacy',\n",
        "    right_on='customer_id_legacy',\n",
        "    how='left'\n",
        ")\n",
        "print(\"\\nTransactions Mapped with New IDs:\")\n",
        "print(transactions_mapped.head())"
      ],
      "metadata": {
        "id": "MXStsO1wWC15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Check if customer_id_new exists\n",
        "if 'customer_id_new' not in transactions_mapped.columns:\n",
        "    raise KeyError(\"Column 'customer_id_new' is missing after merging transactions_df with id_mapping_df.\")\n",
        "\n",
        "# Merge social_profiles_df with id_mapping_df to get customer_id_legacy\n",
        "social_profiles_mapped = pd.merge(\n",
        "    social_profiles_df,\n",
        "    id_mapping_df,\n",
        "    left_on='customer_id_new',\n",
        "    right_on='customer_id_new',\n",
        "    how='left'\n",
        ")\n",
        "print(\"\\nSocial Profiles Mapped with Legacy IDs:\")\n",
        "print(social_profiles_mapped.head())"
      ],
      "metadata": {
        "id": "OH7bTuiWWCys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Check if customer_id_legacy exists\n",
        "if 'customer_id_legacy' not in social_profiles_mapped.columns:\n",
        "    raise KeyError(\"Column 'customer_id_legacy' is missing after merging social_profiles_df with id_mapping_df.\")"
      ],
      "metadata": {
        "id": "UlqZ7hB0WCwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge Both Datasets Based on Transitive Relationships\n",
        "# Merge the two mapped datasets on customer_id_new\n",
        "merged_df = pd.merge(\n",
        "    transactions_mapped,\n",
        "    social_profiles_mapped,\n",
        "    on='customer_id_new',\n",
        "    how='outer'\n",
        ")\n",
        "print(\"\\nFinal Merged Dataset:\")\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "rnNhNULVWCqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Check for missing columns\n",
        "missing_columns = ['customer_id_new', 'customer_id_legacy', 'engagement_score']\n",
        "for col in missing_columns:\n",
        "    if col not in merged_df.columns:\n",
        "        print(f\"Warning: Column '{col}' is missing in the final merged dataset.\")"
      ],
      "metadata": {
        "id": "ezYO0Oj9WOsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle conflicts where one customer ID maps to multiple entries\n",
        "# Aggregate duplicate rows by taking the mean of numerical columns and mode of categorical columns\n",
        "def aggregate_duplicates(df):\n",
        "    # Group by customer_id_new and aggregate\n",
        "    aggregation_dict = {\n",
        "        'purchase_amount': 'mean',\n",
        "        'customer_rating': 'mean',\n",
        "        'engagement_score': lambda x: x.sum() if 'engagement_score' in df.columns else np.nan,\n",
        "        'purchase_interest_score': 'mean'\n",
        "    }\n",
        "\n",
        "    # Only include customer_id_legacy if it exists\n",
        "    if 'customer_id_legacy' in df.columns:\n",
        "        aggregation_dict['customer_id_legacy'] = lambda x: x.mode()[0] if not x.empty else np.nan\n",
        "\n",
        "    aggregated_df = df.groupby('customer_id_new', as_index=False).agg(aggregation_dict)\n",
        "    return aggregated_df\n",
        "\n",
        "merged_df = aggregate_duplicates(merged_df)\n",
        "print(\"\\nAggregated Merged Dataset:\")\n",
        "print(merged_df.head())"
      ],
      "metadata": {
        "id": "C2vEjcf2WOph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Customer Engagement Score\n",
        "# Combine transaction history (purchase_amount) and engagement_score\n",
        "if 'engagement_score' in merged_df.columns:\n",
        "    merged_df['customer_engagement_score'] = (\n",
        "        merged_df['purchase_amount'] * 0.6 +\n",
        "        merged_df['engagement_score'] * 0.4\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: 'engagement_score' column is missing. Skipping customer engagement score calculation.\")\n",
        "    merged_df['customer_engagement_score'] = merged_df['purchase_amount'] * 0.6\n",
        "\n",
        "print(\"\\nCustomer Engagement Score Added:\")\n",
        "print(merged_df[['customer_id_new', 'customer_engagement_score']].head())"
      ],
      "metadata": {
        "id": "8O1uQxR5WOm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engineer predictive behavioral features\n",
        "# Moving average of transactions (rolling window of 3)\n",
        "merged_df.sort_values(by=['customer_id_new'], inplace=True)\n",
        "merged_df['moving_avg_purchase'] = merged_df.groupby('customer_id_new')['purchase_amount'].transform(\n",
        "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        ")\n",
        "print(\"\\nMoving Average of Purchases Added:\")\n",
        "print(merged_df[['customer_id_new', 'purchase_amount', 'moving_avg_purchase']].head())"
      ],
      "metadata": {
        "id": "d5SWu3r4WOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time-based aggregation of purchases (e.g., monthly total)\n",
        "# Convert purchase_date to datetime\n",
        "if 'purchase_date' in merged_df.columns:\n",
        "    merged_df['month'] = merged_df['purchase_date'].dt.to_period('M').astype(str)\n",
        "    monthly_aggregation = merged_df.groupby(['customer_id_new', 'month'])['purchase_amount'].sum().reset_index()\n",
        "    monthly_aggregation.rename(columns={'purchase_amount': 'monthly_total_purchase'}, inplace=True)\n",
        "    merged_df = pd.merge(merged_df, monthly_aggregation, on=['customer_id_new', 'month'], how='left')\n",
        "    print(\"\\nMonthly Aggregation of Purchases Added:\")\n",
        "    print(merged_df[['customer_id_new', 'month', 'monthly_total_purchase']].head())\n",
        "else:\n",
        "    print(\"Warning: 'purchase_date' column is missing. Skipping time-based aggregation.\")\n",
        "\n",
        "# TF-IDF on review_sentiment\n",
        "if 'review_sentiment' in merged_df.columns:\n",
        "    tfidf = TfidfVectorizer(max_features=10)  # Limit to top 10 features for simplicity\n",
        "    tfidf_matrix = tfidf.fit_transform(merged_df['review_sentiment'].fillna(''))\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "    merged_df = pd.concat([merged_df.reset_index(drop=True), tfidf_df], axis=1)\n",
        "    print(\"\\nTF-IDF Features Added:\")\n",
        "    print(merged_df.head())\n",
        "else:\n",
        "    print(\"Warning: 'review_sentiment' column is missing. Skipping TF-IDF feature extraction.\")"
      ],
      "metadata": {
        "id": "EUhDfeNeWOh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the Final Preprocessed Data\n",
        "# Save the final dataset\n",
        "output_file = 'final_customer_data_group17.csv'\n",
        "merged_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\\nFinal Preprocessed Dataset Saved Successfully as '{output_file}'!\")"
      ],
      "metadata": {
        "id": "c2LgMBAiWOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Data Consistency and Quality Checks\n",
        "Objective: Ensure that the preprocessed dataset is clean, structured, and machine learning-ready."
      ],
      "metadata": {
        "id": "AtLyt8ROWlEC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taCypYVWWmaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}